{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Home Credit Default Risk - Data Processing and Feature Engineering\n",
    "\n",
    " It's important to note that most features are derived from domain knowledge and experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = r'C:\\Users\\kresi\\OneDrive\\Desktop\\Turing college\\Project11xx\\data/'\n",
    "APPLICATION_TRAIN = INPUT_DIR + 'application_train.csv'\n",
    "APPLICATION_TEST = INPUT_DIR + 'application_test.csv'\n",
    "BUREAU = INPUT_DIR + 'bureau.csv'\n",
    "BUREAU_BALANCE = INPUT_DIR + 'bureau_balance.csv'\n",
    "PREVIOUS_APPLICATION = INPUT_DIR + 'previous_application.csv'\n",
    "POS_CASH = INPUT_DIR + 'POS_CASH_balance.csv'\n",
    "INSTALLMENTS_PAYMENTS = INPUT_DIR + 'installments_payments.csv'\n",
    "CREDIT_CARD_BALANCE = INPUT_DIR + 'credit_card_balance.csv'\n",
    "\n",
    "OUTPUT_DIR = r'C:\\Users\\kresi\\OneDrive\\Desktop\\Turing college\\Project11xx/'\n",
    "PROCESSED_TRAIN = OUTPUT_DIR + 'processed_train.csv'\n",
    "PROCESSED_TEST = OUTPUT_DIR + 'processed_test.csv'\n",
    "TRAIN_TARGET = OUTPUT_DIR + 'train_target.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Helper Functions\n",
    "\n",
    " Helper functions:\n",
    "- `read_csv`: Reads CSV files\n",
    "- `reduce_mem_usage`: Optimizes memory usage by downcasting numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path, nrows=None):\n",
    "    \"\"\"Read CSV file using pandas.\"\"\"\n",
    "    return pd.read_csv(file_path, nrows=nrows)\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"Reduce memory usage of a dataframe by downcasting numeric columns.\"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(df[col].dtype)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Process Application Data\n",
    "\n",
    " The `process_application` function handles the main application data:\n",
    "\n",
    "1. Reads and reduces memory usage of the dataframe.\n",
    "2. Handles missing and erroneous values.\n",
    "3. Removes unnecessary document flag features.\n",
    "4. Converts days to years for certain features.\n",
    "5. Fills categorical columns with 'XNA' for missing values.\n",
    "6. Performs extensive feature engineering, creating new features based on existing ones.\n",
    "7. Encodes categorical variables using LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_application(file_path, is_train=True):\n",
    "    \"\"\"Process application_{train|test}.csv\"\"\"\n",
    "    df = read_csv(file_path)\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "    df['DAYS_LAST_PHONE_CHANGE'] = df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan)\n",
    "\n",
    "    flag_docs_to_remove = ['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_20']\n",
    "    df = df.drop(flag_docs_to_remove, axis=1)\n",
    "\n",
    "    df['DAYS_BIRTH'] = df['DAYS_BIRTH'] * -1 / 365\n",
    "\n",
    "    df['OBS_30_CNT_SOCIAL_CIRCLE'] = df['OBS_30_CNT_SOCIAL_CIRCLE'].apply(lambda x: np.nan if x > 30 else x)\n",
    "    df['OBS_60_CNT_SOCIAL_CIRCLE'] = df['OBS_60_CNT_SOCIAL_CIRCLE'].apply(lambda x: np.nan if x > 30 else x)\n",
    "\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    df[categorical_columns] = df[categorical_columns].fillna('XNA')\n",
    "\n",
    "    df['REGION_RATING_CLIENT'] = df['REGION_RATING_CLIENT'].astype('object')\n",
    "    df['REGION_RATING_CLIENT_W_CITY'] = df['REGION_RATING_CLIENT_W_CITY'].astype('object')\n",
    "\n",
    "    new_features = {\n",
    "        'CREDIT_INCOME_PERCENT': df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL'],\n",
    "        'ANNUITY_INCOME_PERCENT': df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL'],\n",
    "        'CREDIT_TERM': df['AMT_ANNUITY'] / df['AMT_CREDIT'],\n",
    "        'DAYS_EMPLOYED_PERCENT': df['DAYS_EMPLOYED'] / df['DAYS_BIRTH'],\n",
    "        'INCOME_CREDIT_PERCENTAGE': df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT'],\n",
    "        'INCOME_PER_PERSON': df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS'],\n",
    "        'ANNUITY_LENGTH': df['AMT_CREDIT'] / df['AMT_ANNUITY'],\n",
    "        'CHILDREN_RATIO': df['CNT_CHILDREN'] / df['CNT_FAM_MEMBERS'],\n",
    "        'CREDIT_TO_GOODS_RATIO': df['AMT_CREDIT'] / df['AMT_GOODS_PRICE'],\n",
    "        'INC_PER_CHLD': df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN']),\n",
    "        'SOURCES_PROD': df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3'],\n",
    "        'CAR_TO_BIRTH_RATIO': df['OWN_CAR_AGE'] / df['DAYS_BIRTH'],\n",
    "        'CAR_TO_EMPLOY_RATIO': df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED'],\n",
    "        'PHONE_TO_BIRTH_RATIO': df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH'],\n",
    "        'PHONE_TO_EMPLOY_RATIO': df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
    "    }\n",
    "\n",
    "    new_features.update({\n",
    "        'CREDIT_INCOME_RATIO': df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL'],\n",
    "        'CREDIT_ANNUITY_RATIO': df['AMT_CREDIT'] / df['AMT_ANNUITY'],\n",
    "        'INCOME_ANNUITY_DIFF': df['AMT_INCOME_TOTAL'] - df['AMT_ANNUITY'],\n",
    "        'CREDIT_GOODS_DIFF': df['AMT_CREDIT'] - df['AMT_GOODS_PRICE'],\n",
    "        'GOODS_INCOME_RATIO': df['AMT_GOODS_PRICE'] / df['AMT_INCOME_TOTAL'],\n",
    "        'INCOME_EXT_RATIO': df['AMT_INCOME_TOTAL'] / df['EXT_SOURCE_3'],\n",
    "        'CREDIT_EXT_RATIO': df['AMT_CREDIT'] / df['EXT_SOURCE_3'],\n",
    "        'AGE_EMPLOYED_DIFF': df['DAYS_BIRTH'] - df['DAYS_EMPLOYED'],\n",
    "        'EMPLOYED_TO_AGE_RATIO': df['DAYS_EMPLOYED'] / df['DAYS_BIRTH'],\n",
    "        'CAR_EMPLOYED_DIFF': df['OWN_CAR_AGE'] - df['DAYS_EMPLOYED'],\n",
    "        'CAR_AGE_DIFF': df['DAYS_BIRTH'] - df['OWN_CAR_AGE'],\n",
    "        'CAR_AGE_RATIO': df['OWN_CAR_AGE'] / df['DAYS_BIRTH'],\n",
    "        'FLAG_CONTACTS_SUM': df['FLAG_MOBIL'] + df['FLAG_EMP_PHONE'] + df['FLAG_WORK_PHONE'] + df['FLAG_CONT_MOBILE'] + df['FLAG_PHONE'] + df['FLAG_EMAIL'],\n",
    "        'HOUR_PROCESS_CREDIT_MUL': df['AMT_CREDIT'] * df['HOUR_APPR_PROCESS_START'],\n",
    "        'CNT_NON_CHILDREN': df['CNT_FAM_MEMBERS'] - df['CNT_CHILDREN'],\n",
    "        'CHILDREN_INCOME_RATIO': df['CNT_CHILDREN'] / df['AMT_INCOME_TOTAL'],\n",
    "        'PER_CAPITA_INCOME': df['AMT_INCOME_TOTAL'] / (df['CNT_FAM_MEMBERS'] + 1),\n",
    "        'REGIONS_RATING_INCOME_MUL': (df['REGION_RATING_CLIENT'] + df['REGION_RATING_CLIENT_W_CITY']) * df['AMT_INCOME_TOTAL'] / 2\n",
    "    })\n",
    "\n",
    "    new_features.update({\n",
    "        'REGION_RATING_MAX': df[['REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY']].max(axis=1),\n",
    "        'REGION_RATING_MIN': df[['REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY']].min(axis=1),\n",
    "        'REGION_RATING_MEAN': (df['REGION_RATING_CLIENT'] + df['REGION_RATING_CLIENT_W_CITY']) / 2,\n",
    "        'REGION_RATING_MUL': df['REGION_RATING_CLIENT'] * df['REGION_RATING_CLIENT_W_CITY'],\n",
    "        'FLAG_REGIONS': df['REG_REGION_NOT_LIVE_REGION'] + df['REG_REGION_NOT_WORK_REGION'] + df['LIVE_REGION_NOT_WORK_REGION'] + df['REG_CITY_NOT_LIVE_CITY'] + df['REG_CITY_NOT_WORK_CITY'] + df['LIVE_CITY_NOT_WORK_CITY'],\n",
    "        'EXT_SOURCE_MEAN': df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1),\n",
    "        'EXT_SOURCE_MUL': df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3'],\n",
    "        'EXT_SOURCE_MAX': df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].max(axis=1),\n",
    "        'EXT_SOURCE_MIN': df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].min(axis=1),\n",
    "        'EXT_SOURCE_VAR': df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].var(axis=1),\n",
    "        'WEIGHTED_EXT_SOURCE': df['EXT_SOURCE_1'] * 2 + df['EXT_SOURCE_2'] * 3 + df['EXT_SOURCE_3'] * 4\n",
    "    })\n",
    "\n",
    "    apartment_avg_columns = [col for col in df.columns if 'AVG' in col]\n",
    "    apartment_mode_columns = [col for col in df.columns if 'MODE' in col]\n",
    "    apartment_medi_columns = [col for col in df.columns if 'MEDI' in col]\n",
    "    \n",
    "    for col in apartment_mode_columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    new_features.update({\n",
    "        'APARTMENTS_SUM_AVG': df[apartment_avg_columns].sum(axis=1),\n",
    "        'APARTMENTS_SUM_MODE': df[apartment_mode_columns].sum(axis=1),\n",
    "        'APARTMENTS_SUM_MEDI': df[apartment_medi_columns].sum(axis=1),\n",
    "        'INCOME_APARTMENT_AVG_MUL': df[apartment_avg_columns].sum(axis=1) * df['AMT_INCOME_TOTAL'],\n",
    "        'INCOME_APARTMENT_MODE_MUL': df[apartment_mode_columns].sum(axis=1) * df['AMT_INCOME_TOTAL'],\n",
    "        'INCOME_APARTMENT_MEDI_MUL': df[apartment_medi_columns].sum(axis=1) * df['AMT_INCOME_TOTAL']\n",
    "    })\n",
    "\n",
    "    new_features.update({\n",
    "        'OBS_30_60_SUM': df['OBS_30_CNT_SOCIAL_CIRCLE'] + df['OBS_60_CNT_SOCIAL_CIRCLE'],\n",
    "        'DEF_30_60_SUM': df['DEF_30_CNT_SOCIAL_CIRCLE'] + df['DEF_60_CNT_SOCIAL_CIRCLE'],\n",
    "        'OBS_DEF_30_MUL': df['OBS_30_CNT_SOCIAL_CIRCLE'] * df['DEF_30_CNT_SOCIAL_CIRCLE'],\n",
    "        'OBS_DEF_60_MUL': df['OBS_60_CNT_SOCIAL_CIRCLE'] * df['DEF_60_CNT_SOCIAL_CIRCLE'],\n",
    "        'SUM_OBS_DEF_ALL': df['OBS_30_CNT_SOCIAL_CIRCLE'] + df['DEF_30_CNT_SOCIAL_CIRCLE'] + df['OBS_60_CNT_SOCIAL_CIRCLE'] + df['DEF_60_CNT_SOCIAL_CIRCLE'],\n",
    "        'OBS_30_CREDIT_RATIO': df['AMT_CREDIT'] / (df['OBS_30_CNT_SOCIAL_CIRCLE'] + 0.00001),\n",
    "        'OBS_60_CREDIT_RATIO': df['AMT_CREDIT'] / (df['OBS_60_CNT_SOCIAL_CIRCLE'] + 0.00001),\n",
    "        'DEF_30_CREDIT_RATIO': df['AMT_CREDIT'] / (df['DEF_30_CNT_SOCIAL_CIRCLE'] + 0.00001),\n",
    "        'DEF_60_CREDIT_RATIO': df['AMT_CREDIT'] / (df['DEF_60_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "    })\n",
    "\n",
    "    flag_doc_columns = [col for col in df.columns if col.startswith('FLAG_DOCUMENT') and col not in flag_docs_to_remove]\n",
    "    new_features['SUM_FLAGS_DOCUMENTS'] = df[flag_doc_columns].sum(axis=1)\n",
    "\n",
    "    new_features.update({\n",
    "        'DAYS_DETAILS_CHANGE_MUL': df['DAYS_LAST_PHONE_CHANGE'] * df['DAYS_REGISTRATION'] * df['DAYS_ID_PUBLISH'],\n",
    "        'DAYS_DETAILS_CHANGE_SUM': df['DAYS_LAST_PHONE_CHANGE'] + df['DAYS_REGISTRATION'] + df['DAYS_ID_PUBLISH']\n",
    "    })\n",
    "\n",
    "    enquiry_columns = [col for col in df.columns if col.startswith('AMT_REQ_CREDIT_BUREAU')]\n",
    "    new_features.update({\n",
    "        'AMT_ENQ_SUM': df[enquiry_columns].sum(axis=1),\n",
    "        'ENQ_CREDIT_RATIO': df[enquiry_columns].sum(axis=1) / df['AMT_CREDIT']\n",
    "    })\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(new_features)], axis=1)\n",
    "\n",
    "    df['MISSING_VALS_TOTAL_APP'] = df.isna().sum(axis=1)\n",
    "\n",
    "    if is_train:\n",
    "        y = df['TARGET']\n",
    "        df = df.drop('TARGET', axis=1)\n",
    "        return df, y\n",
    "    else:\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Impute Missing Values\n",
    "\n",
    " Two functions handle missing value imputation:\n",
    "\n",
    "1. `impute_ext_source`:\n",
    "   - Uses LightGBM to predict missing values in EXT_SOURCE columns.\n",
    "   - Encodes categorical variables before model training.\n",
    "\n",
    "2. `knn_impute_target`:\n",
    "   - Uses K-Nearest Neighbors to impute TARGET mean for each application.\n",
    "   - This helps in creating a feature based on similar applications' outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_ext_source(train_df, test_df):\n",
    "    \"\"\"Impute missing values in EXT_SOURCE columns\"\"\"\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    for ext_col in ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']:\n",
    "        columns_to_drop = ['SK_ID_CURR', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n",
    "        if 'TARGET' in train_df.columns:\n",
    "            columns_to_drop.append('TARGET')\n",
    "        \n",
    "        features = [col for col in train_df.columns if col not in columns_to_drop]\n",
    "        \n",
    "        for feature in features:\n",
    "            if train_df[feature].dtype == 'object':\n",
    "                # Combine train and test data for encoding\n",
    "                combined = pd.concat([train_df[feature], test_df[feature]], axis=0)\n",
    "                le.fit(combined.astype(str))\n",
    "                train_df[feature] = le.transform(train_df[feature].astype(str))\n",
    "                test_df[feature] = le.transform(test_df[feature].astype(str))\n",
    "        \n",
    "        X_model = train_df[~train_df[ext_col].isna()][features]\n",
    "        y_model = train_df[~train_df[ext_col].isna()][ext_col]\n",
    "        X_train_missing = train_df[train_df[ext_col].isna()][features]\n",
    "        X_test_missing = test_df[test_df[ext_col].isna()][features]\n",
    "\n",
    "        model = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=32, n_jobs=-1, random_state=42)\n",
    "        model.fit(X_model, y_model)\n",
    "\n",
    "        train_df.loc[train_df[ext_col].isna(), ext_col] = model.predict(X_train_missing)\n",
    "        test_df.loc[test_df[ext_col].isna(), ext_col] = model.predict(X_test_missing)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def knn_impute_target(train_df, test_df):\n",
    "    \"\"\"Impute TARGET mean of 500 nearest neighbors\"\"\"\n",
    "    features_for_knn = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']\n",
    "    \n",
    "    if 'TARGET' not in train_df.columns:\n",
    "        print(\"Warning: 'TARGET' column not found in training data. Skipping KNN imputation.\")\n",
    "        return train_df, test_df\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=500, n_jobs=-1)\n",
    "    train_features = train_df[features_for_knn].fillna(-999)\n",
    "    test_features = test_df[features_for_knn].fillna(-999)\n",
    "    \n",
    "    knn.fit(train_features, train_df['TARGET'])\n",
    "    \n",
    "    train_neighbors = knn.kneighbors(train_features)[1]\n",
    "    test_neighbors = knn.kneighbors(test_features)[1]\n",
    "    \n",
    "    train_df['TARGET_NEIGHBORS_MEAN'] = [train_df['TARGET'].iloc[indices].mean() for indices in train_neighbors]\n",
    "    test_df['TARGET_NEIGHBORS_MEAN'] = [train_df['TARGET'].iloc[indices].mean() for indices in test_neighbors]\n",
    "    \n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Process Bureau Data\n",
    "\n",
    " The `process_bureau` function handles credit bureau data:\n",
    "\n",
    "1. Reads and reduces memory usage of the bureau dataframe.\n",
    "2. Performs feature engineering on bureau data.\n",
    "3. Processes bureau_balance data:\n",
    "   - Converts STATUS to numeric.\n",
    "   - Creates weighted status features.\n",
    "   - Calculates exponential weighted averages.\n",
    "4. Aggregates bureau_balance data.\n",
    "5. Aggregates bureau data, creating summary statistics for each SK_ID_CURR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bureau(file_path):\n",
    "    \"\"\"Process bureau.csv and bureau_balance.csv\"\"\"\n",
    "    bureau = read_csv(file_path)\n",
    "    bureau = reduce_mem_usage(bureau)\n",
    "\n",
    "    bureau = bureau.fillna(0)\n",
    "\n",
    "    bureau['CREDIT_DURATION'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n",
    "    \n",
    "    new_features = {\n",
    "        'FLAG_OVERDUE_RECENT': (bureau['CREDIT_DAY_OVERDUE'] > 0).astype(int),\n",
    "        'MAX_AMT_OVERDUE_DURATION_RATIO': bureau['AMT_CREDIT_MAX_OVERDUE'] / (bureau['CREDIT_DURATION'] + 0.00001),\n",
    "        'CURRENT_AMT_OVERDUE_DURATION_RATIO': bureau['AMT_CREDIT_SUM_OVERDUE'] / (bureau['CREDIT_DURATION'] + 0.00001),\n",
    "        'AMT_OVERDUE_DURATION_LEFT_RATIO': bureau['AMT_CREDIT_SUM_OVERDUE'] / (bureau['DAYS_CREDIT_ENDDATE'] + 0.00001),\n",
    "        'CNT_PROLONGED_MAX_OVERDUE_MUL': bureau['CNT_CREDIT_PROLONG'] * bureau['AMT_CREDIT_MAX_OVERDUE'],\n",
    "        'CNT_PROLONGED_DURATION_RATIO': bureau['CNT_CREDIT_PROLONG'] / (bureau['CREDIT_DURATION'] + 0.00001),\n",
    "        'CURRENT_DEBT_TO_CREDIT_RATIO': bureau['AMT_CREDIT_SUM_DEBT'] / (bureau['AMT_CREDIT_SUM'] + 0.00001),\n",
    "        'CURRENT_CREDIT_DEBT_DIFF': bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT'],\n",
    "        'AMT_ANNUITY_CREDIT_RATIO': bureau['AMT_ANNUITY'] / (bureau['AMT_CREDIT_SUM'] + 0.00001),\n",
    "        'CREDIT_ENDDATE_UPDATE_DIFF': abs(bureau['DAYS_CREDIT_UPDATE'] - bureau['DAYS_CREDIT_ENDDATE'])\n",
    "    }\n",
    "\n",
    "    bureau = pd.concat([bureau, pd.DataFrame(new_features)], axis=1)\n",
    "\n",
    "    bb_file = file_path.replace('bureau.csv', 'bureau_balance.csv')\n",
    "    bb = read_csv(bb_file)\n",
    "    bb = reduce_mem_usage(bb)\n",
    "\n",
    "    status_map = {'C': 0, '0': 1, '1': 2, '2': 3, 'X': 4, '3': 5, '4': 6, '5': 7}\n",
    "    bb['STATUS'] = bb['STATUS'].map(status_map)\n",
    "\n",
    "    bb['MONTHS_BALANCE'] = abs(bb['MONTHS_BALANCE'])\n",
    "    bb['WEIGHTED_STATUS'] = bb['STATUS'] / (bb['MONTHS_BALANCE'] + 1)\n",
    "\n",
    "    bb = bb.sort_values(['SK_ID_BUREAU', 'MONTHS_BALANCE'], ascending=[True, False])\n",
    "    bb['EXP_WEIGHTED_STATUS'] = bb.groupby('SK_ID_BUREAU')['WEIGHTED_STATUS'].transform(lambda x: x.ewm(alpha=0.8).mean())\n",
    "    bb['EXP_ENCODED_STATUS'] = bb.groupby('SK_ID_BUREAU')['STATUS'].transform(lambda x: x.ewm(alpha=0.8).mean())\n",
    "\n",
    "    bb_aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean'],\n",
    "        'STATUS': ['mean', 'max', 'min'],\n",
    "        'WEIGHTED_STATUS': ['mean', 'max', 'min'],\n",
    "        'EXP_WEIGHTED_STATUS': ['last'],\n",
    "        'EXP_ENCODED_STATUS': ['last']\n",
    "    }\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([f'BB_{e[0]}_{e[1].upper()}' for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "\n",
    "    aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum', 'mean', 'max'],\n",
    "        'AMT_CREDIT_SUM': ['sum', 'mean', 'max'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['sum', 'mean', 'max'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['sum', 'mean', 'max'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['sum', 'mean', 'max'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean', 'min', 'max'],\n",
    "        'AMT_ANNUITY': ['sum', 'mean', 'max'],\n",
    "        'CREDIT_DURATION': ['mean', 'max'],\n",
    "        'FLAG_OVERDUE_RECENT': ['sum', 'mean'],\n",
    "        'MAX_AMT_OVERDUE_DURATION_RATIO': ['mean', 'max'],\n",
    "        'CURRENT_AMT_OVERDUE_DURATION_RATIO': ['mean', 'max'],\n",
    "        'AMT_OVERDUE_DURATION_LEFT_RATIO': ['mean', 'max'],\n",
    "        'CNT_PROLONGED_MAX_OVERDUE_MUL': ['mean', 'max'],\n",
    "        'CNT_PROLONGED_DURATION_RATIO': ['mean', 'max'],\n",
    "        'CURRENT_DEBT_TO_CREDIT_RATIO': ['mean', 'max'],\n",
    "        'CURRENT_CREDIT_DEBT_DIFF': ['mean', 'max'],\n",
    "        'AMT_ANNUITY_CREDIT_RATIO': ['mean', 'max'],\n",
    "        'CREDIT_ENDDATE_UPDATE_DIFF': ['mean', 'max'],\n",
    "    }\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    bureau_agg.columns = pd.Index([f'BURO_{e[0]}_{e[1].upper()}' for e in bureau_agg.columns.tolist()])\n",
    "    return bureau_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Process Previous Application Data\n",
    "\n",
    " The `process_previous_application` function handles previous application data:\n",
    "\n",
    "1. Reads and reduces memory usage of the previous application dataframe.\n",
    "2. Handles erroneous values.\n",
    "3. Encodes categorical variables.\n",
    "4. Performs extensive feature engineering.\n",
    "5. Aggregates data, creating summary statistics for each SK_ID_CURR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_previous_application(file_path):\n",
    "    \"\"\"Process previous_application.csv\"\"\"\n",
    "    prev = read_csv(file_path)\n",
    "    prev = reduce_mem_usage(prev)\n",
    "\n",
    "    prev['DAYS_FIRST_DRAWING'] = prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan)\n",
    "    prev['DAYS_FIRST_DUE'] = prev['DAYS_FIRST_DUE'].replace(365243, np.nan)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'] = prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan)\n",
    "    prev['DAYS_LAST_DUE'] = prev['DAYS_LAST_DUE'].replace(365243, np.nan)\n",
    "    prev['DAYS_TERMINATION'] = prev['DAYS_TERMINATION'].replace(365243, np.nan)\n",
    "    prev['SELLERPLACE_AREA'] = prev['SELLERPLACE_AREA'].replace(4000000, np.nan)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    categorical_columns = prev.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        prev[col] = le.fit_transform(prev[col].astype(str))\n",
    "\n",
    "    new_features = {\n",
    "        'MISSING_VALUES_TOTAL_PREV': prev.isnull().sum(axis=1),\n",
    "        'AMT_DECLINED': prev['AMT_APPLICATION'] - prev['AMT_CREDIT'],\n",
    "        'AMT_CREDIT_GOODS_RATIO': prev['AMT_CREDIT'] / (prev['AMT_GOODS_PRICE'] + 0.00001),\n",
    "        'AMT_CREDIT_GOODS_DIFF': prev['AMT_CREDIT'] - prev['AMT_GOODS_PRICE'],\n",
    "        'AMT_CREDIT_APPLICATION_RATIO': prev['AMT_APPLICATION'] / (prev['AMT_CREDIT'] + 0.00001),\n",
    "        'CREDIT_DOWNPAYMENT_RATIO': prev['AMT_DOWN_PAYMENT'] / (prev['AMT_CREDIT'] + 0.00001),\n",
    "        'GOOD_DOWNPAYMET_RATIO': prev['AMT_DOWN_PAYMENT'] / (prev['AMT_GOODS_PRICE'] + 0.00001),\n",
    "        'INTEREST_DOWNPAYMENT': prev['RATE_DOWN_PAYMENT'] * prev['AMT_DOWN_PAYMENT'],\n",
    "        'INTEREST_CREDIT': prev['AMT_CREDIT'] * prev['RATE_INTEREST_PRIMARY'],\n",
    "        'INTEREST_CREDIT_PRIVILEGED': prev['AMT_CREDIT'] * prev['RATE_INTEREST_PRIVILEGED'],\n",
    "        'APPLICATION_AMT_TO_DECISION_RATIO': prev['AMT_APPLICATION'] / (prev['DAYS_DECISION'].abs() + 0.00001),\n",
    "        'AMT_APPLICATION_TO_SELLERPLACE_AREA': prev['AMT_APPLICATION'] / (prev['SELLERPLACE_AREA'] + 0.00001),\n",
    "        'ANNUITY': prev['AMT_CREDIT'] / (prev['CNT_PAYMENT'] + 0.00001),\n",
    "        'ANNUITY_GOODS': prev['AMT_GOODS_PRICE'] / (prev['CNT_PAYMENT'] + 0.00001),\n",
    "        'DAYS_FIRST_LAST_DUE_DIFF': prev['DAYS_LAST_DUE'] - prev['DAYS_FIRST_DUE'],\n",
    "        'AMT_CREDIT_HOUR_PROCESS_START': prev['AMT_CREDIT'] * prev['HOUR_APPR_PROCESS_START'],\n",
    "        'AMT_CREDIT_NFLAG_LAST_APPL_DAY': prev['AMT_CREDIT'] * prev['NFLAG_LAST_APPL_IN_DAY'],\n",
    "        'AMT_CREDIT_YIELD_GROUP': prev['AMT_CREDIT'] * prev['NAME_YIELD_GROUP'],\n",
    "        'AMT_INTEREST': prev['CNT_PAYMENT'] * prev['AMT_ANNUITY'] - prev['AMT_CREDIT'],\n",
    "        'INTEREST_SHARE': (prev['CNT_PAYMENT'] * prev['AMT_ANNUITY'] - prev['AMT_CREDIT']) / (prev['AMT_CREDIT'] + 0.00001),\n",
    "        'INTEREST_RATE': 2 * 12 * (prev['CNT_PAYMENT'] * prev['AMT_ANNUITY'] - prev['AMT_CREDIT']) / (prev['AMT_CREDIT'] * (prev['CNT_PAYMENT'] + 1))\n",
    "    }\n",
    "\n",
    "    prev = pd.concat([prev, pd.DataFrame(new_features)], axis=1)\n",
    "\n",
    "    aggregations = {\n",
    "        'SK_ID_CURR': ['count'],\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        'DAYS_FIRST_DRAWING': ['min', 'max', 'mean'],\n",
    "        'DAYS_FIRST_DUE': ['min', 'max', 'mean'],\n",
    "        'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "        'DAYS_LAST_DUE': ['min', 'max', 'mean'],\n",
    "        'DAYS_TERMINATION': ['min', 'max', 'mean'],\n",
    "        'NFLAG_INSURED_ON_APPROVAL': ['mean'],\n",
    "        'MISSING_VALUES_TOTAL_PREV': ['sum', 'mean'],\n",
    "        'AMT_DECLINED': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_GOODS_RATIO': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_GOODS_DIFF': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_APPLICATION_RATIO': ['min', 'max', 'mean'],\n",
    "        'CREDIT_DOWNPAYMENT_RATIO': ['min', 'max', 'mean'],\n",
    "        'GOOD_DOWNPAYMET_RATIO': ['min', 'max', 'mean'],\n",
    "        'INTEREST_DOWNPAYMENT': ['min', 'max', 'mean'],\n",
    "        'INTEREST_CREDIT': ['min', 'max', 'mean'],\n",
    "        'INTEREST_CREDIT_PRIVILEGED': ['min', 'max', 'mean'],\n",
    "        'APPLICATION_AMT_TO_DECISION_RATIO': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION_TO_SELLERPLACE_AREA': ['min', 'max', 'mean'],\n",
    "        'ANNUITY': ['min', 'max', 'mean'],\n",
    "        'ANNUITY_GOODS': ['min', 'max', 'mean'],\n",
    "        'DAYS_FIRST_LAST_DUE_DIFF': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_HOUR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_NFLAG_LAST_APPL_DAY': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_YIELD_GROUP': ['min', 'max', 'mean'],\n",
    "        'AMT_INTEREST': ['min', 'max', 'mean'],\n",
    "        'INTEREST_SHARE': ['min', 'max', 'mean'],\n",
    "        'INTEREST_RATE': ['min', 'max', 'mean']\n",
    "    }\n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    prev_agg.columns = pd.Index([f'PREV_{e[0]}_{e[1].upper()}' for e in prev_agg.columns.tolist()])\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Process POS CASH Balance Data\n",
    "\n",
    " The `process_pos_cash` function handles POS CASH balance data:\n",
    "\n",
    "1. Reads and reduces memory usage of the POS CASH balance dataframe.\n",
    "2. Performs feature engineering, including exponential moving averages.\n",
    "3. Aggregates data, creating summary statistics for each SK_ID_CURR.\n",
    "4. Creates and aggregates dummy variables for NAME_CONTRACT_STATUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pos_cash(file_path):\n",
    "    \"\"\"Process POS_CASH_balance.csv\"\"\"\n",
    "    pos = read_csv(file_path)\n",
    "    pos = reduce_mem_usage(pos)\n",
    "\n",
    "    pos['MONTHS_BALANCE'] = pos['MONTHS_BALANCE'].abs()\n",
    "\n",
    "    pos = pos.sort_values(['SK_ID_PREV', 'MONTHS_BALANCE'], ascending=[True, False])\n",
    "\n",
    "    new_features = {\n",
    "        'SK_DPD_RATIO': pos['SK_DPD'] / (pos['SK_DPD_DEF'] + 0.00001),\n",
    "        'TOTAL_TERM': pos['CNT_INSTALMENT'] + pos['CNT_INSTALMENT_FUTURE']\n",
    "    }\n",
    "\n",
    "    for col in ['CNT_INSTALMENT', 'CNT_INSTALMENT_FUTURE']:\n",
    "        new_features[f'EXP_{col}'] = pos.groupby('SK_ID_PREV')[col].transform(lambda x: x.ewm(alpha=0.6).mean())\n",
    "\n",
    "    new_features['EXP_POS_TOTAL_TERM'] = new_features['EXP_CNT_INSTALMENT'] + new_features['EXP_CNT_INSTALMENT_FUTURE']\n",
    "\n",
    "    pos = pd.concat([pos, pd.DataFrame(new_features)], axis=1)\n",
    "\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max'],\n",
    "        'CNT_INSTALMENT': ['mean', 'max', 'min'],\n",
    "        'CNT_INSTALMENT_FUTURE': ['mean', 'max', 'min'],\n",
    "        'SK_DPD': ['max', 'sum'],\n",
    "        'SK_DPD_DEF': ['max', 'sum'],\n",
    "        'SK_DPD_RATIO': ['mean', 'max'],\n",
    "        'TOTAL_TERM': ['mean', 'max', 'last'],\n",
    "        'EXP_CNT_INSTALMENT': ['last'],\n",
    "        'EXP_CNT_INSTALMENT_FUTURE': ['last'],\n",
    "        'EXP_POS_TOTAL_TERM': ['mean']\n",
    "    }\n",
    "\n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index([f'POS_{e[0]}_{e[1].upper()}' for e in pos_agg.columns.tolist()])\n",
    "\n",
    "    pos_dummies = pd.get_dummies(pos['NAME_CONTRACT_STATUS'], prefix='POS_STATUS')\n",
    "    \n",
    "    pos = pd.concat([pos, pos_dummies], axis=1)\n",
    "    \n",
    "    dummy_agg = pos.groupby('SK_ID_CURR')[pos_dummies.columns].mean()\n",
    "    \n",
    "    pos_agg = pd.concat([pos_agg, dummy_agg], axis=1)\n",
    "\n",
    "    return pos_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Process Installments Payments Data\n",
    "\n",
    " The `process_installments_payments` function handles installments payments data:\n",
    "\n",
    "1. Reads and reduces memory usage of the installments payments dataframe.\n",
    "2. Performs feature engineering related to payment behavior.\n",
    "3. Aggregates data, creating summary statistics for each SK_ID_CURR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def process_installments_payments(file_path):\n",
    "    \"\"\"Process installments_payments.csv\"\"\"\n",
    "    ins = read_csv(file_path)\n",
    "    ins = reduce_mem_usage(ins)\n",
    "\n",
    "    # Feature engineering\n",
    "    new_features = {\n",
    "        'MISSING_VALS_TOTAL_CC': ins.isnull().sum(axis=1),\n",
    "        'PAYMENT_PERC': ins['AMT_PAYMENT'] / (ins['AMT_INSTALMENT'] + 0.00001),\n",
    "        'PAYMENT_DIFF': ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT'],\n",
    "        'DPD': (ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']).apply(lambda x: max(x, 0)),\n",
    "        'DBD': (ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']).apply(lambda x: max(x, 0))\n",
    "    }\n",
    "\n",
    "    ins = pd.concat([ins, pd.DataFrame(new_features)], axis=1)\n",
    "\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum'],\n",
    "        'MISSING_VALS_TOTAL_CC': ['sum']\n",
    "    }\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INS_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    return ins_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Process Credit Card Balance Data\n",
    "\n",
    " The `process_credit_card_balance` function handles credit card balance data:\n",
    "\n",
    "1. Reads and reduces memory usage of the credit card balance dataframe.\n",
    "2. Handles erroneous values.\n",
    "3. Performs feature engineering related to credit card usage and payments.\n",
    "4. Calculates exponential weighted moving averages for several features.\n",
    "5. Aggregates data, creating summary statistics for each SK_ID_CURR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_credit_card_balance(file_path):\n",
    "    \"\"\"Process credit_card_balance.csv\"\"\"\n",
    "    cc = read_csv(file_path)\n",
    "    cc = reduce_mem_usage(cc)\n",
    "\n",
    "    cc.loc[cc['AMT_PAYMENT_CURRENT'] > 4000000, 'AMT_PAYMENT_CURRENT'] = np.nan\n",
    "\n",
    "    new_features = {\n",
    "        'MISSING_VALS_TOTAL_CC': cc.isnull().sum(axis=1),\n",
    "        'AMT_DRAWING_SUM': cc['AMT_DRAWINGS_ATM_CURRENT'] + cc['AMT_DRAWINGS_CURRENT'] + cc['AMT_DRAWINGS_OTHER_CURRENT'] + cc['AMT_DRAWINGS_POS_CURRENT'],\n",
    "        'BALANCE_LIMIT_RATIO': cc['AMT_BALANCE'] / (cc['AMT_CREDIT_LIMIT_ACTUAL'] + 0.00001),\n",
    "        'CNT_DRAWING_SUM': cc['CNT_DRAWINGS_ATM_CURRENT'] + cc['CNT_DRAWINGS_CURRENT'] + cc['CNT_DRAWINGS_OTHER_CURRENT'] + cc['CNT_DRAWINGS_POS_CURRENT'] + cc['CNT_INSTALMENT_MATURE_CUM'],\n",
    "        'MIN_PAYMENT_RATIO': cc['AMT_PAYMENT_CURRENT'] / (cc['AMT_INST_MIN_REGULARITY'] + 0.00001),\n",
    "        'PAYMENT_MIN_DIFF': cc['AMT_PAYMENT_CURRENT'] - cc['AMT_INST_MIN_REGULARITY'],\n",
    "        'MIN_PAYMENT_TOTAL_RATIO': cc['AMT_PAYMENT_TOTAL_CURRENT'] / (cc['AMT_INST_MIN_REGULARITY'] + 0.00001),\n",
    "        'AMT_INTEREST_RECEIVABLE': cc['AMT_TOTAL_RECEIVABLE'] - cc['AMT_RECEIVABLE_PRINCIPAL'],\n",
    "        'SK_DPD_RATIO': cc['SK_DPD'] / (cc['SK_DPD_DEF'] + 0.00001)\n",
    "    }\n",
    "\n",
    "    cc = pd.concat([cc, pd.DataFrame(new_features)], axis=1)\n",
    "\n",
    "    for col in ['AMT_BALANCE', 'AMT_CREDIT_LIMIT_ACTUAL', 'AMT_RECEIVABLE_PRINCIPAL', 'AMT_RECIVABLE', 'AMT_TOTAL_RECEIVABLE', \n",
    "                'AMT_DRAWING_SUM', 'BALANCE_LIMIT_RATIO', 'CNT_DRAWING_SUM', 'MIN_PAYMENT_RATIO', 'PAYMENT_MIN_DIFF', \n",
    "                'MIN_PAYMENT_TOTAL_RATIO', 'AMT_INTEREST_RECEIVABLE', 'SK_DPD_RATIO']:\n",
    "        cc[f'EXP_{col}'] = cc.groupby('SK_ID_PREV')[col].transform(lambda x: x.ewm(alpha=0.7).mean())\n",
    "\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'AMT_BALANCE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL': ['max', 'mean'],\n",
    "        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'mean'],\n",
    "        'AMT_DRAWINGS_CURRENT': ['max', 'mean'],\n",
    "        'AMT_DRAWINGS_OTHER_CURRENT': ['max', 'mean'],\n",
    "        'AMT_DRAWINGS_POS_CURRENT': ['max', 'mean'],\n",
    "        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n",
    "        'AMT_PAYMENT_CURRENT': ['max', 'mean'],\n",
    "        'AMT_PAYMENT_TOTAL_CURRENT': ['max', 'mean'],\n",
    "        'AMT_RECEIVABLE_PRINCIPAL': ['max', 'mean'],\n",
    "        'AMT_RECIVABLE': ['max', 'mean'],\n",
    "        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': ['max', 'mean'],\n",
    "        'CNT_DRAWINGS_CURRENT': ['max', 'mean'],\n",
    "        'CNT_DRAWINGS_OTHER_CURRENT': ['max', 'mean'],\n",
    "        'CNT_DRAWINGS_POS_CURRENT': ['max', 'mean'],\n",
    "        'CNT_INSTALMENT_MATURE_CUM': ['max', 'mean'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean'],\n",
    "        'AMT_DRAWING_SUM': ['max', 'mean'],\n",
    "        'BALANCE_LIMIT_RATIO': ['max', 'mean'],\n",
    "        'CNT_DRAWING_SUM': ['max', 'mean'],\n",
    "        'MIN_PAYMENT_RATIO': ['min', 'mean'],\n",
    "        'PAYMENT_MIN_DIFF': ['min', 'mean'],\n",
    "        'MIN_PAYMENT_TOTAL_RATIO': ['min', 'mean'],\n",
    "        'AMT_INTEREST_RECEIVABLE': ['min', 'mean'],\n",
    "        'SK_DPD_RATIO': ['max', 'mean'],\n",
    "        'MISSING_VALS_TOTAL_CC': ['sum'],\n",
    "    }\n",
    "\n",
    "    for col in cc.columns:\n",
    "        if col.startswith('EXP_'):\n",
    "            aggregations[col] = ['last']\n",
    "\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    return cc_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Main Processing Function\n",
    "\n",
    " The `main` function orchestrates the entire data processing pipeline:\n",
    "\n",
    "1. Processes application data for both train and test sets.\n",
    "2. Imputes missing values in EXT_SOURCE columns and performs KNN-based TARGET imputation.\n",
    "3. Processes and joins data from all other datasets (bureau, previous application, POS CASH balance, installments payments, credit card balance).\n",
    "4. Handles any remaining missing values by filling with mean values.\n",
    "5. Saves the processed datasets as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Processing application_train.csv and application_test.csv...\")\n",
    "    train_df, y = process_application(APPLICATION_TRAIN, is_train=True)\n",
    "    test_df = process_application(APPLICATION_TEST, is_train=False)\n",
    "\n",
    "    print(\"Imputing missing values in EXT_SOURCE columns...\")\n",
    "    train_df, test_df = impute_ext_source(train_df, test_df)\n",
    "\n",
    "    print(\"Imputing TARGET mean of 500 nearest neighbors...\")\n",
    "    train_df, test_df = knn_impute_target(train_df, test_df)\n",
    "\n",
    "    print(\"Processing bureau.csv and bureau_balance.csv...\")\n",
    "    bureau_agg = process_bureau(BUREAU)\n",
    "    train_df = train_df.join(bureau_agg, how='left', on='SK_ID_CURR')\n",
    "    test_df = test_df.join(bureau_agg, how='left', on='SK_ID_CURR')\n",
    "    del bureau_agg\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Processing previous_application.csv...\")\n",
    "    prev_agg = process_previous_application(PREVIOUS_APPLICATION)\n",
    "    train_df = train_df.join(prev_agg, how='left', on='SK_ID_CURR')\n",
    "    test_df = test_df.join(prev_agg, how='left', on='SK_ID_CURR')\n",
    "    del prev_agg\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Processing POS_CASH_balance.csv...\")\n",
    "    pos_agg = process_pos_cash(POS_CASH)\n",
    "    train_df = train_df.join(pos_agg, how='left', on='SK_ID_CURR')\n",
    "    test_df = test_df.join(pos_agg, how='left', on='SK_ID_CURR')\n",
    "    del pos_agg\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Processing installments_payments.csv...\")\n",
    "    ins_agg = process_installments_payments(INSTALLMENTS_PAYMENTS)\n",
    "    train_df = train_df.join(ins_agg, how='left', on='SK_ID_CURR')\n",
    "    test_df = test_df.join(ins_agg, how='left', on='SK_ID_CURR')\n",
    "    del ins_agg\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Processing credit_card_balance.csv...\")\n",
    "    cc_agg = process_credit_card_balance(CREDIT_CARD_BALANCE)\n",
    "    train_df = train_df.join(cc_agg, how='left', on='SK_ID_CURR')\n",
    "    test_df = test_df.join(cc_agg, how='left', on='SK_ID_CURR')\n",
    "    del cc_agg\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Final preprocessing steps...\")\n",
    "    # Handle remaining missing values\n",
    "    train_df = train_df.fillna(train_df.mean())\n",
    "    test_df = test_df.fillna(test_df.mean())\n",
    "\n",
    "    print(\"Saving processed datasets...\")\n",
    "    train_df.to_csv(PROCESSED_TRAIN, index=False)\n",
    "    test_df.to_csv(PROCESSED_TEST, index=False)\n",
    "    train_target = pd.DataFrame({'SK_ID_CURR': train_df['SK_ID_CURR'], 'TARGET': y})\n",
    "    train_target.to_csv(TRAIN_TARGET, index=False)\n",
    "\n",
    "    print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Run the Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing application_train.csv and application_test.csv...\n",
      "Imputing missing values in EXT_SOURCE columns...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23146\n",
      "[LightGBM] [Info] Number of data points in the train set: 134133, number of used features: 172\n",
      "[LightGBM] [Info] Start training from score 0.502129\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23281\n",
      "[LightGBM] [Info] Number of data points in the train set: 306851, number of used features: 172\n",
      "[LightGBM] [Info] Start training from score 0.514393\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23162\n",
      "[LightGBM] [Info] Number of data points in the train set: 246546, number of used features: 172\n",
      "[LightGBM] [Info] Start training from score 0.510852\n",
      "Imputing TARGET mean of 500 nearest neighbors...\n",
      "Warning: 'TARGET' column not found in training data. Skipping KNN imputation.\n",
      "Processing bureau.csv and bureau_balance.csv...\n",
      "Processing previous_application.csv...\n",
      "Processing POS_CASH_balance.csv...\n",
      "Processing installments_payments.csv...\n",
      "Processing credit_card_balance.csv...\n",
      "Final preprocessing steps...\n",
      "Saving processed datasets...\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Conclusion\n",
    "\n",
    "\n",
    "\n",
    " This notebook has processed the Home Credit Default Risk dataset, including:\n",
    "\n",
    "\n",
    "\n",
    " 1. Data cleaning and initial feature engineering for each dataset\n",
    "\n",
    " 2. Imputation of missing values, especially for EXT_SOURCE columns\n",
    "\n",
    " 3. KNN-based imputation for TARGET-related features\n",
    "\n",
    " 4. Aggregation of data from multiple related tables\n",
    "\n",
    " 5. Final imputation of any remaining missing values\n",
    "\n",
    "\n",
    "\n",
    " The processed datasets are saved as CSV files for further analysis and modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
